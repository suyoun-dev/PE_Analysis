{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03) 어간 추출(Stemming) and 표제어 추출(Lemmatization)\n",
    "\n",
    "단어 형태 변화에 의해 다른 단어로 인식이 되나 그 Root를 찾아서 하나의 단어로 일반화하는 작업\n",
    "\n",
    "\n",
    "- 어간 | stem : 한국어에서 동사, 영어는 명사, 동사, 변하지 않는 부분\n",
    "\n",
    "- 어미 | ending : 변하는 부분\n",
    "\n",
    "예: 달리다, 달리는, 달리고, 달리던, ....\n",
    "\n",
    "\n",
    "\n",
    "- 어근 | root: 변하지 않는 부분\n",
    "\n",
    "- 접사 | affix: (변하는 부분) 어근에 붙어서 어근의 내용을 제한하는 부분(prefix, suffix, infix)\n",
    "\n",
    "예: 햇과일/과일, 짓누르다/누르다\n",
    "\n",
    "- 어간 추출: stemming\n",
    "\n",
    "규칙 기반, pos 고려하지 않음. 대부분 결과가 부정확\n",
    "\n",
    "-s, s지워라 => hypothesis -> hypothesi\n",
    "\n",
    "smiling(n) -> smile(v) => pos tag info가 저자오디지 않는다.\n",
    "  \n",
    "\n",
    "- 표제어 추출: lemmatization\n",
    "\n",
    "wordnet 사전을 활용. pos info 활용. smile(n)\n",
    "\n",
    "#### 이유/효과\n",
    "1) 연산량이 줄어든다.(작업효율성 높아짐) 2) 효율성 높아짐\n",
    "\n",
    "### 1. 표제어 추출(Lemmatization)\n",
    "\n",
    "What is Lemma?\n",
    "\n",
    "am, are, is -> they are all from \"be\". So \"be\" is lemma.\n",
    "\n",
    "Lemma를 추출하기 위해서는 단어의 형태학적 파싱을 해야한다.\n",
    "\n",
    "What is morph? -> 의미를 가진 가장 작은 단위\n",
    "1) Stem(어간) - 단어의 의미를 담고 있는 핵심 부분\n",
    "2) Affix(접사) - 단어에 추가적인 의미를 주는 부분\n",
    "\n",
    "-> 단어를 이 두가지 구성 요소로 분리하는 작업을 형태학적 파싱이라고 한다.\n",
    "\n",
    "ex) cats -> cat(stem) + s(suffix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before extracting lemma :  ['hypothese', 'policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
      "After extracting lemma :  ['hypothese', 'policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Erin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = ['hypothese','policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "\n",
    "print(\"Before extracting lemma : \", words)\n",
    "print(\"After extracting lemma : \", [lemmatizer.lemmatize(word) for word in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lemmatization은 어간 추출과 달리 단어의 형태가 어느정도 보존이 된다.\n",
    "\n",
    "하지만 die의 dy, has의 ha처럼 이상하게 출력되는 경우가 있는데\n",
    "\n",
    "이는 lemmatizer가 단어의 품사 정보를 알아야 그를 바탕으로 정확한 결과를 얻을 수 있기 때문이다.\n",
    "\n",
    "WordNetLemmatizer는 단어와 단어의 품사를 정확히 입력하면 그에 맞게 lemma를 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'die'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('dies', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'watch'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('watched', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'have'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('have', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hypothese'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('hypothese', 'n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 어간 추출 Stemming\n",
    "\n",
    "정해진 규칙을 보고 접사를 제거하는 작업\n",
    "\n",
    "섬세한 작업은 아니기 때문에 사전에 존재하지 않는 단어가 나올 수도 있음.\n",
    "\n",
    "ex) Porter's Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어간 추출 전 : ['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n",
      "어간 추출 후 : ['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "sentence = \"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
    "tokenized_sentence = word_tokenize(sentence)\n",
    "\n",
    "print('어간 추출 전 :', tokenized_sentence)\n",
    "print('어간 추출 후 :',[stemmer.stem(word) for word in tokenized_sentence])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porter's Algorithm은 규칙 기반 중 일부\n",
    "\n",
    "-ALIZE → AL -ANCE → 제거 -ICAL → IC\n",
    "\n",
    "ex) formalize → formal  allowance → allow electricical → electric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어간 추출 전 : ['formalize', 'allowance', 'electricical', 'hypothese']\n",
      "어간 추출 후 : ['formal', 'allow', 'electric', 'hypothes']\n"
     ]
    }
   ],
   "source": [
    "words = ['formalize', 'allowance', 'electricical', 'hypothese']\n",
    "\n",
    "print('어간 추출 전 :',words)\n",
    "print('어간 추출 후 :',[stemmer.stem(word) for word in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porter's algorithm외에 Lancaster Stemmer도 있다. 이 둘은 서로 다른 알고리즘으로 진행하기 때문에 결과가 다르게 나온다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어간 추출 전 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
      "포터 스테머의 어간 추출 후: ['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n",
      "랭커스터 스테머의 어간 추출 후: ['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "\n",
    "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
    "print('어간 추출 전 :', words)\n",
    "print('포터 스테머의 어간 추출 후:',[porter_stemmer.stem(w) for w in words])\n",
    "print('랭커스터 스테머의 어간 추출 후:',[lancaster_stemmer.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cooking', 'cooked', 'cookery', 'cooker', 'cooks']\n",
      "Porter:  ['cook', 'cook', 'cookeri', 'cooker', 'cook']\n",
      "Lancaster:  ['cook', 'cook', 'cookery', 'cook', 'cook']\n"
     ]
    }
   ],
   "source": [
    "word1 = 'cooking cooked cookery cooker cooks'.split()\n",
    "print(word1)\n",
    "print(\"Porter: \", [porter_stemmer.stem(w) for w in word1])\n",
    "print(\"Lancaster: \", [lancaster_stemmer.stem(w) for w in word1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       [Word]           [Lancaster]           [Porter]      \n",
      "       friend              friend              friend       \n",
      "     friendship            friend            friendship     \n",
      "      friends              friend              friend       \n",
      "    friendships            friend            friendship     \n",
      "       stable              stabl               stabl        \n",
      "    destabilize             dest              destabil      \n",
      "  misunderstanding     misunderstand       misunderstand    \n",
      "      railroad            railroad            railroad      \n",
      "     moonlight           moonlight           moonlight      \n",
      "      football            footbal             footbal       \n"
     ]
    }
   ],
   "source": [
    "word_list = 'friend friendship friends friendships \\\n",
    "stable destabilize misunderstanding railroad moonlight football'.split()\n",
    "\n",
    "print('{0:^20}{1:^20}{2:^20}'.format('[Word]', '[Lancaster]', '[Porter]'))\n",
    "for word in word_list:\n",
    "    # 각각에 20개씩 스페이스를 줄 것\n",
    "    print('{0:^20}{1:^20}{2:^20}'.format(word, lancaster_stemmer.stem(word), porter_stemmer.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming:  ['am', 'the go', 'have']\n",
      "Lemmatization:  ['be', 'the going', 'have']\n"
     ]
    }
   ],
   "source": [
    "words = ['am', 'the going', 'having']\n",
    "print(\"Stemming: \", [porter_stemmer.stem(w) for w in words])\n",
    "print(\"Lemmatization: \", [lemmatizer.lemmatize(w, 'v') for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 한국어에서의 어간 추출\n",
    "\n",
    "* 체언 - 명사, 대명사, 수사\n",
    "* 수식언 - 관형사, 부사\n",
    "* 관계언 - 조사\n",
    "* 독립언 - 감탄사\n",
    "* 용언 - 동사, 형용사 -> 어간+어미\n",
    "\n",
    "\n",
    "1) 활용(conjugation) = 어간, 어미를 가짐.\n",
    "- 어간 | stem : 한국어에서 동사, 영어는 명사, 동사, 변하지 않는 부분\n",
    "\n",
    "- 어미 | ending : 변하는 부분\n",
    "\n",
    "예: 달리다, 달리는, 달리고, 달리던, ....\n",
    "\n",
    "\n",
    "\n",
    "- 어근 | root: 변하지 않는 부분\n",
    "\n",
    "- 접사 | affix: (변하는 부분) 어근에 붙어서 어근의 내용을 제한하는 부분(prefix, suffix, infix)\n",
    "\n",
    "예: 햇과일/과일, 짓누르다/누르다\n",
    "\n",
    "활용은 어간이 어미를 취할 때, 어간의 모습이 일정하다면 규칙 활용, 어간이나 어미의 모습이 변하는 불규칙 활용으로 나뉜다.\n",
    "\n",
    "ex) 규칙 활용\n",
    "\n",
    "잡/어간 다/어미 - 어미 붙기전 과 붙은 후의 모습이 같음.\n",
    "\n",
    "ex) 불규칙 활용\n",
    "\n",
    "듣/들-, 돕-/도우-, 곱/고우- ......"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ce209477809c92873094b2238512cf85b57bea0d3911a3451defb67f61f5a97"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
