{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "01) 토큰화 Tokenization\n",
    "\n",
    "주어진 코퍼스에서 토큰이라는 단위로 나누는 작업\n",
    "\n",
    "토큰이란 단어 단위 외에 단어구, 의미를 갖는 문자열 등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Erin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화1 : ['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "# word_tokenize는 Don't -> Do, n't / Jone's -> Jone, 's\n",
    "print('단어 토큰화1 :', word_tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화2 : ['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "# WordPunctTokenizer는 Don't -> Don, ', t / Jone's -> Jone, ', s\n",
    "# WordPunctTokenzier는 구두점을 별도로 분류하는 특징을 가지고 있음.\n",
    "print('단어 토큰화2 :',WordPunctTokenizer().tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화3 : [\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "# keras는 Don't -> don't / Jone's -> jone's\n",
    "# keras의 text_to_word_sequence는 기본적으로 input을 모두 소문자화, 구두점 제거함. 하지만 '는 유지\n",
    "print('단어 토큰화3 :',text_to_word_sequence(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "토큰화에서 고려해야할 사항\n",
    "1) 구두점이나 특수문자의 제거 여부 ex) $45.60, ph.D, AT&T 등..\n",
    "2) 줄임말이나 단어 내에 띄어쓰기가 있는 경우 ex) New York, rock'n roll, we're\n",
    "3) 표준 토큰화 예제 - TreebankWordTokenizer ex) home-based, doesn't -> does/n't\n",
    "\n",
    "표준 토큰화 예제 - Penn Treebank Tokenization\n",
    "rule 1. 하이픈으로 구성된 단어는 하나로 유지\n",
    "rule 2. doesn't 같이 apostrophe로 '접어'와 함께하는 단어는 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "트리뱅크 워드토크나이저 : ['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "text = \"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\"\n",
    "print('트리뱅크 워드토크나이저 :',tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문장 토큰화(Sentence Tokenization)\n",
    "\n",
    "문장을 구분하는 기준? ?나 !는 가능하지만 마침표의 경우 꼭 문장의 끝에만 쓰이는 것은 아니기 때문에 좀 더 생각해봐야한다.\n",
    "\n",
    "EX1) IP 192.168.56.31 서버에 들어가서 로그 파일 저장해서 aaa@gmail.com로 결과 좀 보내줘. 그 후 점심 먹으러 가자.\n",
    "\n",
    "EX2) Since I'm actively looking for Ph.D. students, I get the same question a dozen times every year.\n",
    "\n",
    "사용하는 코퍼스가 어떤 국적의 언어인지, 또는 해당 코퍼스 내에서 특수문자를 어떻게 사용하고 있는지에 따라 규칙 정의 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 토큰화1 : ['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text =  \"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\"\n",
    "print('문장 토큰화1 :',sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 토큰화2 : ['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']\n"
     ]
    }
   ],
   "source": [
    "# Ph.D를 단어로 인식. 마침표로 구분하지 않음.\n",
    "text = \"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\n",
    "print('문장 토큰화2 :',sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kssNote: you may need to restart the kernel to use updated packages.\n",
      "  Downloading kss-3.4.tar.gz (42.4 MB)\n",
      "Collecting emoji\n",
      "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
      "Requirement already satisfied: regex in c:\\users\\erin\\anaconda3\\lib\\site-packages (from kss) (2021.8.3)\n",
      "Building wheels for collected packages: kss, emoji\n",
      "  Building wheel for kss (setup.py): started\n",
      "  Building wheel for kss (setup.py): finished with status 'done'\n",
      "  Created wheel for kss: filename=kss-3.4-py3-none-any.whl size=42449208 sha256=1ed7722555028020444369e70328791a2e44201ef93797c02d75dce0ba2c2458\n",
      "  Stored in directory: c:\\users\\erin\\appdata\\local\\pip\\cache\\wheels\\58\\0e\\23\\604858eb7ac6d054c94de431343707a1b4b9ee36f3543c93bb\n",
      "  Building wheel for emoji (setup.py): started\n",
      "  Building wheel for emoji (setup.py): finished with status 'done'\n",
      "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171047 sha256=86d374af0d489e625aaf915db2cc03b0413ab2d80d5a85dccab73d4464d3b0dd\n",
      "  Stored in directory: c:\\users\\erin\\appdata\\local\\pip\\cache\\wheels\\fa\\7a\\e9\\22dd0515e1bad255e51663ee513a2fa839c95934c5fc301090\n",
      "Successfully built kss emoji\n",
      "Installing collected packages: emoji, kss\n",
      "Successfully installed emoji-1.7.0 kss-3.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install kss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한국어 KSS(Korean Sentence Splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Korean Sentence Splitter]: Initializing Pynori...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국어 문장 토큰화 : ['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다.', '이제 해보면 알걸요?']\n"
     ]
    }
   ],
   "source": [
    "import kss\n",
    "\n",
    "text = '딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다. 이제 해보면 알걸요?'\n",
    "print('한국어 문장 토큰화 :',kss.split_sentences(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한국어에서의 토큰화의 어려움\n",
    "\n",
    "1) 어절(띄어쓰기 단위)와 단어 토큰화가 같지 않음 -> 이는 교착어 때문\n",
    "\n",
    "ex) '그' + 조사: 가, 를, 에게, 와, 는, ....\n",
    "\n",
    "조사가 붙으면 다른 단어로 인식이 되기 때문에 분리해야함. 영어에서는 띄어쓰기로 나뉠 수 있지만 한글을 그렇지 않음.\n",
    "\n",
    "* 자립 형태소: 접사, 어미, 조사와 상관없이 자립하여 사용할 수 있는 ㅎ여태소. 그 자체가 단어가 된다. 체언(명사, 대명사, 수사), 수식언(관형사, 부사), 감탄사 등\n",
    "* 의존 형태소 : 다른 형태소와 결합하여 사용되는 형태소, 접사, 어미, 조사, 어간\n",
    "\n",
    "문장 : 에디가 책을 읽었다\n",
    "\n",
    "<띄어쓰기 단위>\n",
    "\n",
    " ['에디가', '책을', '읽었다']\n",
    "\n",
    "<형태소 단위>\n",
    "\n",
    "자립 형태소 : 에디, 책\n",
    "\n",
    "의존 형태소 : -가, -을, 읽-, -었, -다\n",
    "\n",
    "다음과 같이 '에디'라는 사람 이름과 '책'이라는 명사를 추출할 수 있음.\n",
    "\n",
    "2) 한글은 띄어쓰기가 영어보다 잘 지켜지지 않음.\n",
    "\n",
    "한글은 띄어쓰기를 안해도 쉽게 이해할 수 있기 때문...\n",
    "\n",
    "EX1) 제가이렇게띄어쓰기를전혀하지않고글을썼다고하더라도글을이해할수있습니다.\n",
    "\n",
    "EX2) Tobeornottobethatisthequestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "품사 태깅(Part-of-speech)\n",
    "\n",
    "단어가 어떤 품사로 쓰였는지에 따라 의미, 토큰화, 해석 여부가 달라지기 때문에 중요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Erin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화 :  ['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n",
      "품사 태깅 : [('I', 'PRP'), ('am', 'VBP'), ('actively', 'RB'), ('looking', 'VBG'), ('for', 'IN'), ('Ph.D.', 'NNP'), ('students', 'NNS'), ('.', '.'), ('and', 'CC'), ('you', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('Ph.D.', 'NNP'), ('student', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "text = \"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
    "tokenized_sentence = word_tokenize(text)\n",
    "\n",
    "print('단어 토큰화 : ', tokenized_sentence)\n",
    "print('품사 태깅 :',pos_tag(tokenized_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting konlpy\n",
      "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
      "Collecting JPype1>=0.7.0\n",
      "  Downloading JPype1-1.3.0-cp39-cp39-win_amd64.whl (362 kB)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\users\\erin\\anaconda3\\lib\\site-packages (from konlpy) (4.6.3)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\users\\erin\\anaconda3\\lib\\site-packages (from konlpy) (1.20.3)\n",
      "Installing collected packages: JPype1, konlpy\n",
      "Successfully installed JPype1-1.3.0 konlpy-0.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OKT 형태소 분석:  ['허물', '벗지', '못', '하는', '뱀', '은', '파멸한다', '.', '의견', '바꾸는', '것', '을', '훼방', '놓는', '정신', '들', '도', '마찬가지', '다', '.', '그것', '들', '은', '더', '이상', '정신', '이', '아니다']\n",
      "OKT 품사 태깅 :  [('허물', 'Noun'), ('벗지', 'Verb'), ('못', 'VerbPrefix'), ('하는', 'Verb'), ('뱀', 'Noun'), ('은', 'Josa'), ('파멸한다', 'Adjective'), ('.', 'Punctuation'), ('의견', 'Noun'), ('바꾸는', 'Verb'), ('것', 'Noun'), ('을', 'Josa'), ('훼방', 'Noun'), ('놓는', 'Verb'), ('정신', 'Noun'), ('들', 'Suffix'), ('도', 'Josa'), ('마찬가지', 'Noun'), ('다', 'Josa'), ('.', 'Punctuation'), ('그것', 'Noun'), ('들', 'Suffix'), ('은', 'Josa'), ('더', 'Noun'), ('이상', 'Noun'), ('정신', 'Noun'), ('이', 'Josa'), ('아니다', 'Adjective')]\n",
      "OKT 명사 추출 :  ['허물', '뱀', '의견', '것', '훼방', '정신', '마찬가지', '그것', '더', '이상', '정신']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "from konlpy.tag import Kkma\n",
    "\n",
    "okt = Okt()\n",
    "kkma = Kkma()\n",
    "\n",
    "print(\"OKT 형태소 분석: \", okt.morphs(\"허물 벗지 못하는 뱀은 파멸한다. 의견 바꾸는 것을 훼방 놓는 정신들도 마찬가지다. 그것들은 더 이상 정신이 아니다\"))\n",
    "print(\"OKT 품사 태깅 : \", okt.pos(\"허물 벗지 못하는 뱀은 파멸한다. 의견 바꾸는 것을 훼방 놓는 정신들도 마찬가지다. 그것들은 더 이상 정신이 아니다\"))\n",
    "print(\"OKT 명사 추출 : \", okt.nouns(\"허물 벗지 못하는 뱀은 파멸한다. 의견 바꾸는 것을 훼방 놓는 정신들도 마찬가지다. 그것들은 더 이상 정신이 아니다\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "꼬꼬마 형태소 분석:  ['허물', '벗', '지', '못하', '는', '뱀', '은', '파멸', '하', 'ㄴ다', '.', '의견', '바꾸', '는', '것', '을', '훼방', '놓', '는', '정신', '들', '도', '마찬가지', '이', '다', '.', '그것', '들', '은', '더', '이상', '정신', '이', '아니', '다']\n",
      "꼬꼬마 품사 태깅 :  [('허물', 'NNG'), ('벗', 'VV'), ('지', 'ECD'), ('못하', 'VX'), ('는', 'ETD'), ('뱀', 'NNG'), ('은', 'JX'), ('파멸', 'NNG'), ('하', 'XSV'), ('ㄴ다', 'EFN'), ('.', 'SF'), ('의견', 'NNG'), ('바꾸', 'VV'), ('는', 'ETD'), ('것', 'NNB'), ('을', 'JKO'), ('훼방', 'NNG'), ('놓', 'VV'), ('는', 'ETD'), ('정신', 'NNG'), ('들', 'XSN'), ('도', 'JX'), ('마찬가지', 'NNG'), ('이', 'VCP'), ('다', 'EFN'), ('.', 'SF'), ('그것', 'NP'), ('들', 'XSN'), ('은', 'JX'), ('더', 'MAG'), ('이상', 'NNG'), ('정신', 'NNG'), ('이', 'JKC'), ('아니', 'VCN'), ('다', 'EFN')]\n",
      "꼬꼬마 명사 추출 :  ['허물', '뱀', '파멸', '의견', '훼방', '정신', '마찬가지', '그것', '이상']\n"
     ]
    }
   ],
   "source": [
    "print(\"꼬꼬마 형태소 분석: \", kkma.morphs(\"허물 벗지 못하는 뱀은 파멸한다. 의견 바꾸는 것을 훼방 놓는 정신들도 마찬가지다. 그것들은 더 이상 정신이 아니다\"))\n",
    "print(\"꼬꼬마 품사 태깅 : \", kkma.pos(\"허물 벗지 못하는 뱀은 파멸한다. 의견 바꾸는 것을 훼방 놓는 정신들도 마찬가지다. 그것들은 더 이상 정신이 아니다\"))\n",
    "print(\"꼬꼬마 명사 추출 : \", kkma.nouns(\"허물 벗지 못하는 뱀은 파멸한다. 의견 바꾸는 것을 훼방 놓는 정신들도 마찬가지다. 그것들은 더 이상 정신이 아니다\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8b18d159b453e8648a897f59b4dbc8f778ef99442df851711f546ebc0ae49bbd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
